{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "oJx9t_NTeui1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_kn-XhTetbZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout, Lambda, BatchNormalization, Reshape, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.cluster import KMeans\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from scipy.stats import uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/food_classes_edited_twice.csv\", na_values=[\"<NA>\", \"nan\", \"Nill\", \"Nil\"])\n",
        "df = df.head(25000)  # Use more data if available\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    df['uom_criteria'].fillna(method='ffill', inplace=True)\n",
        "    df['conversion'].fillna(method='ffill', inplace=True)\n",
        "    df['price_new'].fillna(df['price_new'].mean(), inplace=True)\n",
        "    df['price_uom'].fillna(df['price_uom'].mean(), inplace=True)\n",
        "    df.drop(['dob_new', 'age_group', 'Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    categorical_cols = ['item_type', 'class_name', 'subclass_name', 'customer_type', 'standard_uom', 'class_name_uom']\n",
        "    for col in categorical_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # One-hot encode the 'nova' column\n",
        "    ohe = OneHotEncoder(sparse=False)\n",
        "    nova_encoded = ohe.fit_transform(df[['nova']])\n",
        "    nova_columns = [f'nova_{i}' for i in range(nova_encoded.shape[1])]\n",
        "    df[nova_columns] = nova_encoded\n",
        "    df.drop('nova', axis=1, inplace=True)\n",
        "\n",
        "    df['original_price'] = df['price_new']\n",
        "    scaler = StandardScaler()\n",
        "    numerical_cols = ['price_new', 'conversion', 'price_uom']\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "    return df\n",
        "\n",
        "# Feature Engineering\n",
        "def engineer_features(df):\n",
        "    df['price_per_unit'] = df['price_new'] / df['conversion']\n",
        "    df['health_score'] = df['nova_0'] * 3 + df['nova_1'] * 2 + df['nova_2'] * 1 + df['nova_3'] * 0\n",
        "    df['price_category'] = pd.qcut(df['price_new'], q=5, labels=[1, 2, 3, 4, 5])\n",
        "    df['is_brand'] = df['description'].str.contains('brand', case=False).astype(int)\n",
        "    return df\n",
        "\n",
        "# Main process\n",
        "df = preprocess_data(df)\n",
        "df = engineer_features(df)\n",
        "\n",
        "print(f\"Number of unique transactions: {df['transaction_id'].nunique()}\")\n",
        "print(f\"Number of unique items: {df['description'].nunique()}\")\n",
        "\n",
        "# Market Basket Analysis\n",
        "transactions = df.groupby('transaction_id')['description'].apply(list).values.tolist()\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "frequent_itemsets = apriori(transaction_df, min_support=0.01, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
        "rules = rules.sort_values('lift', ascending=False)\n",
        "\n",
        "# BERT Fine-tuning\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_base = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the descriptions\n",
        "descriptions = df['description'].tolist()\n",
        "encoded_inputs = tokenizer(descriptions, padding=True, truncation=True, return_tensors=\"tf\", max_length=128)\n",
        "\n",
        "# Get the actual sequence length from the encoded inputs\n",
        "seq_length = encoded_inputs['input_ids'].shape[1]\n",
        "\n",
        "# Define the model using functional API\n",
        "def build_attention_model(n_items, embedding_size, feature_size, n_classes):\n",
        "    input_target = Input((1,), name='input_target')\n",
        "    input_context = Input((1,), name='input_context')\n",
        "    input_features = Input((feature_size,), name='input_features')\n",
        "\n",
        "    embedding = Embedding(n_items, embedding_size, input_length=1, name='embedding')\n",
        "    target_embedding = embedding(input_target)\n",
        "    context_embedding = embedding(input_context)\n",
        "\n",
        "    target = Flatten()(target_embedding)\n",
        "    context = Flatten()(context_embedding)\n",
        "\n",
        "    target = Reshape((1, embedding_size))(target)\n",
        "    context = Reshape((1, embedding_size))(context)\n",
        "\n",
        "    attention = MultiHeadAttention(num_heads=4, key_dim=embedding_size)\n",
        "    attn_out = attention(query=target, key=context, value=context)\n",
        "    attn_out = Flatten()(attn_out)\n",
        "    attn_out = LayerNormalization()(attn_out)\n",
        "\n",
        "    concat = Concatenate()([attn_out, input_features])\n",
        "    hidden = Dense(128, activation='relu')(concat)\n",
        "    hidden = Dropout(0.55)(hidden)  # Dropout layer with 50% dropout rate\n",
        "    hidden = Dense(64, activation='relu')(hidden)\n",
        "    hidden = Dropout(0.55)(hidden)  # Another Dropout layer\n",
        "    output = Dense(n_classes, activation='softmax')(hidden)\n",
        "\n",
        "    model = Model(inputs=[input_target, input_context, input_features], outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Prepare data for the model\n",
        "n_items = df['description'].nunique()\n",
        "embedding_size = 32\n",
        "feature_size = df[['price_new', 'price_per_unit', 'health_score', 'price_category', 'is_brand']].shape[1]\n",
        "n_classes = 4  # number of nova classes\n",
        "\n",
        "# Create item pairs\n",
        "item_to_index = {item: idx for idx, item in enumerate(df['description'].unique())}\n",
        "index_to_item = {idx: item for item, idx in item_to_index.items()}\n",
        "\n",
        "item_pairs = []\n",
        "item_features = []\n",
        "item_labels = []\n",
        "\n",
        "for _, group in df.groupby('transaction_id'):\n",
        "    items = group['description'].tolist()\n",
        "    for i in range(len(items)):\n",
        "        for j in range(len(items)):\n",
        "            if i != j:\n",
        "                item_pairs.append([item_to_index[items[i]], item_to_index[items[j]]])\n",
        "                item_features.append(group[['price_new', 'price_per_unit', 'health_score', 'price_category', 'is_brand']].iloc[i].values)\n",
        "                item_labels.append(group[['nova_0', 'nova_1', 'nova_2', 'nova_3']].iloc[i].values)\n",
        "\n",
        "item_pairs = np.array(item_pairs)\n",
        "item_features = np.array(item_features)\n",
        "item_labels = np.array(item_labels)\n",
        "\n",
        "# Building and training the model\n",
        "model = build_attention_model(n_items, embedding_size, feature_size, n_classes)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "# Training within cross-validation loop\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "\n",
        "for train_index, val_index in kf.split(item_pairs):\n",
        "    X_train, X_val = item_pairs[train_index], item_pairs[val_index]\n",
        "    f_train, f_val = item_features[train_index], item_features[val_index]\n",
        "    y_train, y_val = item_labels[train_index], item_labels[val_index]\n",
        "\n",
        "    model.fit([X_train[:, 0], X_train[:, 1], f_train], y_train,\n",
        "              epochs=5, batch_size=32,\n",
        "              validation_data=([X_val[:, 0], X_val[:, 1], f_val], y_val),\n",
        "              verbose=1, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "    score = model.evaluate([X_val[:, 0], X_val[:, 1], f_val], y_val, verbose=0)\n",
        "    cv_scores.append(score[1])\n",
        "\n",
        "print(f\"Cross-validation accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "# Extract item embeddings\n",
        "item_embeddings = model.get_layer('embedding').get_weights()[0]\n",
        "\n",
        "# Function to get recommendations with relevance calculation\n",
        "def cosine_similarity(a, b):\n",
        "    return 1 - cosine(a, b)\n",
        "\n",
        "def get_recommendations(item_id, budget, top_n=5):\n",
        "    item_rules = rules[rules['antecedents'].apply(lambda x: item_id in x)]\n",
        "    item_idx = item_to_index[item_id]\n",
        "    item_embedding = item_embeddings[item_idx]\n",
        "    similarities = np.array([cosine_similarity(item_embedding, emb) for emb in item_embeddings])\n",
        "\n",
        "    recommendations = []\n",
        "    total_cost = 0\n",
        "    considered_items = set()\n",
        "    total_relevance = 0\n",
        "\n",
        "    for _, rule in item_rules.iterrows():\n",
        "        for item in rule['consequents']:\n",
        "            if item not in considered_items and item != item_id:\n",
        "                considered_items.add(item)\n",
        "                item_price = df[df['description'] == item]['original_price'].iloc[0]\n",
        "                if total_cost + item_price <= budget:\n",
        "                    recommendations.append((item, similarities[item_to_index[item]]))\n",
        "                    total_cost += item_price\n",
        "                    total_relevance += similarities[item_to_index[item]]\n",
        "                    if len(recommendations) == top_n:\n",
        "                        avg_relevance = total_relevance / top_n\n",
        "                        return [r[0] for r in sorted(recommendations, key=lambda x: x[1], reverse=True)], total_cost, avg_relevance\n",
        "\n",
        "    for item_idx in similarities.argsort()[::-1]:\n",
        "        item = index_to_item[item_idx]\n",
        "        if item not in considered_items and item != item_id:\n",
        "            considered_items.add(item)\n",
        "            item_price = df[df['description'] == item]['original_price'].iloc[0]\n",
        "            if total_cost + item_price <= budget:\n",
        "                recommendations.append((item, similarities[item_idx]))\n",
        "                total_cost += item_price\n",
        "                total_relevance += similarities[item_idx]\n",
        "                if len(recommendations) == top_n:\n",
        "                    break\n",
        "\n",
        "    avg_relevance = total_relevance / len(recommendations) if recommendations else 0\n",
        "    return [r[0] for r in sorted(recommendations, key=lambda x: x[1], reverse=True)], total_cost, avg_relevance\n",
        "\n",
        "# Example usage\n",
        "item_id = df['description'].iloc[0]  # Example item ID\n",
        "budget = 500  # Example budget\n",
        "recommendations, total_cost, avg_relevance = get_recommendations(item_id, budget)\n",
        "print(f\"\\nItem: {item_id}\")\n",
        "print(f\"Recommended items: {recommendations}\")\n",
        "print(f\"Total cost: ${total_cost:.2f}\")\n",
        "print(f\"Average relevance: {avg_relevance:.4f}\")\n",
        "\n",
        "# Analyze errors\n",
        "def analyze_errors(model, X_test, f_test, y_test, df, item_to_index, index_to_item):\n",
        "    y_pred = model.predict([X_test[:, 0], X_test[:, 1], f_test])\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    errors = np.where(y_pred_classes != y_true_classes)[0]\n",
        "    for idx in errors[:10]:  # Analyze first 10 errors\n",
        "        target_item = index_to_item[X_test[idx, 0]]\n",
        "        context_item = index_to_item[X_test[idx, 1]]\n",
        "        true_class = y_true_classes[idx]\n",
        "        pred_class = y_pred_classes[idx]\n",
        "        print(f\"Error: Target: {target_item}, Context: {context_item}\")\n",
        "        print(f\"True class: {true_class}, Predicted class: {pred_class}\")\n",
        "        print(\"Features:\", f_test[idx])\n",
        "        print()\n",
        "\n",
        "# Prepare test data for error analysis\n",
        "X_train, X_test, f_train, f_test, y_train, y_test = train_test_split(item_pairs, item_features, item_labels, test_size=0.2, random_state=42)\n",
        "model = build_attention_model(n_items, embedding_size, feature_size, n_classes)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit([X_train[:, 0], X_train[:, 1], f_train], y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "analyze_errors(model, X_test, f_test, y_test, df, item_to_index, index_to_item)\n",
        "\n"
      ]
    }
  ]
}